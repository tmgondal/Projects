{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adadb96e",
   "metadata": {},
   "source": [
    "# Step-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d63d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets \n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b8796e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66f009",
   "metadata": {},
   "source": [
    "# Step-2: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc283ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83001f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = get_dataset_config_names(\"subjqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7225987b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books', 'electronics', 'grocery', 'movies', 'restaurants', 'tripadvisor']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a3fbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5da1d9",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f92730a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset subjqa/books (download: 10.86 MiB, generated: 3.42 MiB, post-processed: Unknown size, total: 14.27 MiB) to C:\\Users\\Taimoor M. Gondal\\.cache\\huggingface\\datasets\\subjqa\\books\\1.1.0\\e5588f9298ff2d70686a00cc377e4bdccf4e32287459e3c6baf2dc5ab57fe7fd...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset subjqa downloaded and prepared to C:\\Users\\Taimoor M. Gondal\\.cache\\huggingface\\datasets\\subjqa\\books\\1.1.0\\e5588f9298ff2d70686a00cc377e4bdccf4e32287459e3c6baf2dc5ab57fe7fd. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c27a9407354d48bf52ab6f631fee78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subjqa = load_dataset(\"subjqa\", name=\"books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9645059",
   "metadata": {},
   "source": [
    "**For our use case, we’ll focus on building a QA system for the \"Electronics\" domain. To\n",
    "download the electronics subset, we just need to pass this value to the name argu‐\n",
    "ment of the load_dataset() function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f4d6a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['the subject matter would be interesting'], 'answer_start': [262], 'answer_subj_level': [1], 'ans_subj_score': [0.4166666567325592], 'is_ans_subjective': [False]}\n"
     ]
    }
   ],
   "source": [
    "print(subjqa[\"train\"][\"answers\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c2f9b",
   "metadata": {},
   "source": [
    "**Like other question answering datasets on the Hub, SubjQA stores the answers to\n",
    "each question as a nested dictionary. For example, if we inspect one of the rows in the\n",
    "answers column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34dae55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs={}\n",
    "for split, dset in subjqa.flatten().items():\n",
    "    dfs[split]=dset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef00be",
   "metadata": {},
   "source": [
    "Flatten these nested columns with the flatten() method and convert each split to a Pandas\n",
    "DataFrame as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df42b133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in train: 1314\n",
      "Number of questions in test: 345\n",
      "Number of questions in validation: 256\n"
     ]
    }
   ],
   "source": [
    "for split, df in dfs.items():\n",
    "     print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9db3e105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>answers.text</th>\n",
       "      <th>answers.answer_start</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0312576463</td>\n",
       "      <td>Why none is likeable?</td>\n",
       "      <td>[The characters were not likable, and it often...</td>\n",
       "      <td>[119, 156]</td>\n",
       "      <td>It's the classic &amp;#34;disconnected from parent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0307265439</td>\n",
       "      <td>What is the sentiment of the story?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>I rate books on how much of an impact they hav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title                             question  \\\n",
       "463  0312576463                Why none is likeable?   \n",
       "284  0307265439  What is the sentiment of the story?   \n",
       "\n",
       "                                          answers.text answers.answer_start  \\\n",
       "463  [The characters were not likable, and it often...           [119, 156]   \n",
       "284                                                 []                   []   \n",
       "\n",
       "                                               context  \n",
       "463  It's the classic &#34;disconnected from parent...  \n",
       "284  I rate books on how much of an impact they hav...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_cols = [\"title\", \"question\", \"answers.text\",\n",
    " \"answers.answer_start\", \"context\"]\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c93f9",
   "metadata": {},
   "source": [
    "**Let’s focus on these columns and take a look at a few of the training examples. We can\n",
    "use the sample() method to select a random sample:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c504b803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The characters were not likable, and it often felt forced and a little pretentious'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
    "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
    "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6063d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08afe5",
   "metadata": {},
   "source": [
    "# Step-3: Check Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afdcffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in question_types:\n",
    " counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcb69812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'What': 265,\n",
       " 'How': 712,\n",
       " 'Is': 113,\n",
       " 'Does': 54,\n",
       " 'Do': 86,\n",
       " 'Was': 15,\n",
       " 'Where': 42,\n",
       " 'Why': 25}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03c7565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is the choice?\n",
      "How is the child?\n",
      "How do you like the end?\n",
      "What you can infer about life from this story ?\n",
      "What do you think about book?\n",
      "What is author?\n",
      "Is the story appealing to adults?\n",
      "Is it a good book?\n",
      "Is the story true?\n"
     ]
    }
   ],
   "source": [
    "for question_type in [\"How\", \"What\", \"Is\"]:\n",
    "    for question in (dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)].sample(n=3, random_state=42)['question']):\n",
    "        print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f078682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae4c88",
   "metadata": {},
   "source": [
    "# Step-4: Tokenizer\n",
    "To encode our texts, we’ll load the MiniLM model checkpoint from the Hugging Face\n",
    "Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a7550f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad046f9832544359dec26df0f33875b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/107 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4460b6efe141c591ea61289b9616e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93988e4611b04033bbb3c554f1abd0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9539500f2f414e16bdcea70992d12a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5b866bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How much music can this hold?\"\n",
    "context = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on \\\n",
    "file size.\"\"\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53a105",
   "metadata": {},
   "source": [
    "**In this tutorial, we will extract answers from small passage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05c95476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how much music can this hold? [SEP] an mp3 is about 1 mb / minute, so about 6000 hours depending on file size. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92413a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c9230",
   "metadata": {},
   "source": [
    "# Step-5: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dac8e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea00251666545ceb57590fcfb310162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/127M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14ceb45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9862, -4.7750, -5.4025, -5.2378, -5.2863, -5.5117, -4.9819, -6.1880,\n",
      "         -0.9862,  0.2596, -0.2144, -1.7136,  3.7806,  4.8561, -1.0546, -3.9097,\n",
      "         -1.7374, -4.5944, -1.4278,  3.9949,  5.0391, -0.2018, -3.0193, -4.8549,\n",
      "         -2.3107, -3.5110, -3.5713, -0.9862]]), end_logits=tensor([[-0.9623, -5.4733, -5.0326, -5.1639, -5.4278, -5.5151, -5.1749, -4.6233,\n",
      "         -0.9623, -3.7855, -0.8715, -3.7745, -3.0161, -1.1780,  0.1758, -2.7365,\n",
      "          4.8934,  0.3046, -3.1761, -3.2762,  0.8937,  5.6606, -0.3623, -4.9554,\n",
      "         -3.2531, -0.0914,  1.6211, -0.9623]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697adbb",
   "metadata": {},
   "source": [
    "**Two ways to complete the remaining work**<br>\n",
    "**1) Conventional way**<br>\n",
    "**2) HuggingFace Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f1fa002",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cec486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs shape: {inputs.input_ids.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf4deaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logits shape: torch.Size([1, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Start logits shape: {start_logits.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e45a3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End logits shape: torch.Size([1, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"End logits shape: {end_logits.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1913a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = torch.argmax(start_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3610fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_idx = torch.argmax(end_logits) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a7a6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99dd5b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = tokenizer.decode(answer_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b538d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much music can this hold?\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d32b2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 6000 hours\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b18e2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48919a2d",
   "metadata": {},
   "source": [
    "**HuggingFace Pipeline to complete the work with only model tokenizer, question and context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eacf0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d27e80aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taimoor M. Gondal\\anaconda3\\envs\\aistudio\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.26516205072402954,\n",
       "  'start': 38,\n",
       "  'end': 48,\n",
       "  'answer': '6000 hours'},\n",
       " {'score': 0.22082941234111786,\n",
       "  'start': 16,\n",
       "  'end': 48,\n",
       "  'answer': '1 MB/minute, so about 6000 hours'},\n",
       " {'score': 0.10253491997718811,\n",
       "  'start': 16,\n",
       "  'end': 27,\n",
       "  'answer': '1 MB/minute'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(question=question, context=context, topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4e46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42786d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8416e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935b697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66919d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdcb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01feab66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
